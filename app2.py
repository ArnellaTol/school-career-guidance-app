import streamlit as st
import pandas as pd
import numpy as np
import joblib

# import matplotlib.pyplot as plt
from streamlit_option_menu import option_menu

st.set_page_config(
    page_title="–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤",
)


out_col_names=[
 'signsystem',
 'technology',
 'nature',
 'artistic',
 'human',
 'business']

inp_col_names=[
 'kaz_lang_7', 'liter_7', 'rus_lang_7', 'eng_lang_7', 'math_7', 'comps_7', 
 'kaz_hist_7', 'art_7', 'pe_7', 'geography_7', 'biology_7', 'chemistry_7', 
 'physics_7', 'world_hist_7', 'Activist', 'Career', 'Tester', 'Creator', 
 'Designer', 'Researcher', 'kaz_lang_8', 'liter_8', 'rus_lang_8', 'eng_lang_8', 
 'math_8', 'comps_8', 'kaz_hist_8', 'art_8', 'pe_8', 'geography_8', 'biology_8', 
 'chemistry_8', 'physics_8', 'world_hist_8', 'kaz_lang_9', 'liter_9', 'rus_lang_9', 
 'eng_lang_9', 'math_9', 'comps_9', 'kaz_hist_9', 'art_9', 'pe_9', 'geography_9', 
 'biology_9', 'chemistry_9', 'physics_9', 'world_hist_9', 'rights_9', 'kaz_lang_10', 
 'liter_10', 'rus_lang_10', 'eng_lang_10', 'math_10', 'comps_10', 'kaz_hist_10', 
 'art_10', 'pe_10', 'geography_10', 'biology_10', 'chemistry_10', 'physics_10', 
 'world_hist_10'
]

column_names_dict = {
    'kaz_lang_7': 'Kazakh Language',
    'liter_7': 'Literature',
    'rus_lang_7': 'Russian Language',
    'eng_lang_7': 'English Language',
    'math_7': 'Mathematics',
    'comps_7': 'Informatics',
    'kaz_hist_7': 'History of Kazakhstan',
    'art_7': 'Art',
    'pe_7': 'Physical Education',
    'geography_7': 'Geography',
    'biology_7': 'Biology',
    'chemistry_7': 'Chemistry',
    'physics_7': 'Physics',
    'world_hist_7': 'World History',
    'Activist': 'Activist',
    'Career': 'Careerist',
    'Tester': 'Tester',
    'Creator': 'Creator',
    'Designer': 'Designer',
    'Researcher': 'Researcher',
    'kaz_lang_8': 'Kazakh Language',
    'liter_8': 'Literature',
    'rus_lang_8': 'Russian Language',
    'eng_lang_8': 'English Language',
    'math_8': 'Mathematics',
    'comps_8': 'Informatics',
    'kaz_hist_8': 'History of Kazakhstan',
    'art_8': 'Art',
    'pe_8': 'Physical Education',
    'geography_8': 'Geography',
    'biology_8': 'Biology',
    'chemistry_8': 'Chemistry',
    'physics_8': 'Physics',
    'world_hist_8': 'World History',
    'kaz_lang_9': 'Kazakh Language',
    'liter_9': 'Literature',
    'rus_lang_9': 'Russian Language',
    'eng_lang_9': 'English Language',
    'math_9': 'Mathematics',
    'comps_9': 'Informatics',
    'kaz_hist_9': 'History of Kazakhstan',
    'art_9': 'Art',
    'pe_9': 'Physical Education',
    'geography_9': 'Geography',
    'biology_9': 'Biology',
    'chemistry_9': 'Chemistry',
    'physics_9': 'Physics',
    'world_hist_9': 'World History',
    'rights_9': 'Law Fundamentals',
    'kaz_lang_10': 'Kazakh Language',
    'liter_10': 'Literature',
    'rus_lang_10': 'Russian Language',
    'eng_lang_10': 'English Language',
    'math_10': 'Mathematics',
    'comps_10': 'Informatics',
    'kaz_hist_10': 'History of Kazakhstan',
    'art_10': 'Art',
    'pe_10': 'Physical Education',
    'geography_10': 'Geography',
    'biology_10': 'Biology',
    'chemistry_10': 'Chemistry',
    'physics_10': 'Physics',
    'world_hist_10': 'World History'
}


column_names_dict_ru = {
    'kaz_lang_7': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫',
    'liter_7': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
    'rus_lang_7': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫',
    'eng_lang_7': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫',
    'math_7': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
    'comps_7': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞',
    'kaz_hist_7': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞',
    'art_7': '–ò—Å–∫—É—Å—Å—Ç–≤–æ',
    'pe_7': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞',
    'geography_7': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è',
    'biology_7': '–ë–∏–æ–ª–æ–≥–∏—è',
    'chemistry_7': '–•–∏–º–∏—è',
    'physics_7': '–§–∏–∑–∏–∫–∞',
    'world_hist_7': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è',
    'Activist': '–ê–∫—Ç–∏–≤–∏—Å—Ç',
    'Career': '–ö–∞—Ä—å–µ—Ä–∏—Å—Ç',
    'Tester': '–ò—Å–ø—ã—Ç–∞—Ç–µ–ª—å',
    'Creator': '–¢–≤–æ—Ä–µ—Ü',
    'Designer': '–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫',
    'Researcher': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å',
    'kaz_lang_8': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫',
    'liter_8': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
    'rus_lang_8': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫',
    'eng_lang_8': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫',
    'math_8': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
    'comps_8': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞',
    'kaz_hist_8': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞',
    'art_8': '–ò—Å–∫—É—Å—Å—Ç–≤–æ',
    'pe_8': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞',
    'geography_8': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è',
    'biology_8': '–ë–∏–æ–ª–æ–≥–∏—è',
    'chemistry_8': '–•–∏–º–∏—è',
    'physics_8': '–§–∏–∑–∏–∫–∞',
    'world_hist_8': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è',
    'kaz_lang_9': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫',
    'liter_9': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
    'rus_lang_9': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫',
    'eng_lang_9': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫',
    'math_9': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
    'comps_9': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞',
    'kaz_hist_9': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞',
    'art_9': '–ò—Å–∫—É—Å—Å—Ç–≤–æ',
    'pe_9': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞',
    'geography_9': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è',
    'biology_9': '–ë–∏–æ–ª–æ–≥–∏—è',
    'chemistry_9': '–•–∏–º–∏—è',
    'physics_9': '–§–∏–∑–∏–∫–∞',
    'world_hist_9': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è',
    'rights_9': '–û—Å–Ω–æ–≤—ã –ø—Ä–∞–≤–∞',
    'kaz_lang_10': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫',
    'liter_10': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
    'rus_lang_10': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫',
    'eng_lang_10': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫',
    'math_10': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
    'comps_10': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞',
    'kaz_hist_10': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞',
    'art_10': '–ò—Å–∫—É—Å—Å—Ç–≤–æ',
    'pe_10': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞',
    'geography_10': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è',
    'biology_10': '–ë–∏–æ–ª–æ–≥–∏—è',
    'chemistry_10': '–•–∏–º–∏—è',
    'physics_10': '–§–∏–∑–∏–∫–∞',
    'world_hist_10': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è'
}

checkbox_columns = [
    'Activist', 'Career', 'Tester', 'Creator', 'Designer', 'Researcher'
]

def create_expander(class_label, cols):
    # with st.expander(f"Grades for the {class_label}th grade"):
    with st.expander(f"–û—Ü–µ–Ω–∫–∏ –∑–∞ {class_label} –∫–ª–∞—Å—Å"):
        for col in cols:
            # column_names_dict_ru - column_names_dict
            input_values[col] = st.number_input(column_names_dict_ru[col], min_value=2, max_value=5, step=1, value=5, key=col)

input_values = {}


def save_to_dataframe(selected_checkboxes, input_values):
    data = {**selected_checkboxes, **input_values}
    
    for key in ['Activist', 'Career', 'Tester', 'Creator', 'Designer', 'Researcher']:
        data[key] = int(data.get(key, False))
    
    column_order = [
        'kaz_lang_7', 'liter_7', 'rus_lang_7', 'eng_lang_7', 'math_7', 'comps_7', 
        'kaz_hist_7', 'art_7', 'pe_7', 'geography_7', 'biology_7', 'chemistry_7', 
        'physics_7', 'world_hist_7', 'Activist', 'Career', 'Tester', 'Creator', 
        'Designer', 'Researcher', 'kaz_lang_8', 'liter_8', 'rus_lang_8', 'eng_lang_8', 
        'math_8', 'comps_8', 'kaz_hist_8', 'art_8', 'pe_8', 'geography_8', 'biology_8', 
        'chemistry_8', 'physics_8', 'world_hist_8', 'kaz_lang_9', 'liter_9', 'rus_lang_9', 
        'eng_lang_9', 'math_9', 'comps_9', 'kaz_hist_9', 'art_9', 'pe_9', 'geography_9', 
        'biology_9', 'chemistry_9', 'physics_9', 'world_hist_9', 'rights_9', 'kaz_lang_10', 
        'liter_10', 'rus_lang_10', 'eng_lang_10', 'math_10', 'comps_10', 'kaz_hist_10', 
        'art_10', 'pe_10', 'geography_10', 'biology_10', 'chemistry_10', 'physics_10', 
        'world_hist_10'
    ]
    
    df = pd.DataFrame([data], columns=column_order)
    
    return df




import logging

logging.basicConfig(level=logging.DEBUG)

def apply_model(model_path, input_df):
    logging.debug("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏")
    model = joblib.load(model_path)
    
    logging.debug("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –¥–∞–Ω–Ω—ã–º")
    probabilities = model.predict_proba(input_df)
    
    logging.debug(f"–¢–∏–ø probabilities: {type(probabilities)}")
    logging.debug(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ probabilities: {probabilities}")
    
    if isinstance(probabilities, list):
        probabilities = np.array(probabilities)
    
    logging.debug(f"–¢–∏–ø –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {type(probabilities)}")
    
    probability_dict = {f'class_{i}': probabilities[i][:, 1] for i in range(len(probabilities))}
    
    logging.debug("–°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
    result_df = pd.DataFrame(probability_dict)
    
    return result_df





type_columns = {
    'class_0': 'Person-Sign System',
    'class_1': 'Person-Technology',
    'class_2': 'Person-Nature',
    'class_3': 'Person-Artistic Image',
    'class_4': 'Person-Person',
    'class_5': 'Person-Business'
}

type_columns_ru = {
    'class_0': '–ß–µ–ª–æ–≤–µ–∫-–ó–Ω–∞–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞',
    'class_1': '–ß–µ–ª–æ–≤–µ–∫-–¢–µ—Ö–Ω–∏–∫–∞',
    'class_2': '–ß–µ–ª–æ–≤–µ–∫-–ü—Ä–∏—Ä–æ–¥–∞',
    'class_3': '–ß–µ–ª–æ–≤–µ–∫-–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑',
    'class_4': '–ß–µ–ª–æ–≤–µ–∫-–ß–µ–ª–æ–≤–µ–∫',
    'class_5': '–ß–µ–ª–æ–≤–µ–∫-–ë–∏–∑–Ω–µ—Å'
}

thresholds = {
    'class_0': 0.39,
    'class_1': 0.30903005409623036,
    'class_2': 0.23611111111111113,
    'class_3': 0.44833333333333336,
    'class_4': 0.13,
    'class_5': 0.17
}



def adjust_probabilities(probabilities, thresholds):
    adjusted_probs = {}
    for key, value in probabilities.items():
        adjusted_probs[key] = min(100, (value / thresholds[key]) * 100)
    return adjusted_probs




def display_results(df):
    results = {key: df[key].values[0] for key in df.columns}
    adjusted_results = adjust_probabilities(results, thresholds)

    # type_columns_ru - type_columns
    selected_types = [
        type_columns_ru[key] 
        for key, value in adjusted_results.items() if value >= 100
    ]

    st.write("**–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–∏–ø—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏, –≤—ã–≤–µ–¥–µ–Ω–Ω—ã–µ –î–∂. –•–æ–ª–ª–∞–Ω–¥–æ–º:**")
    for typ in selected_types:
        st.write(f"- {typ}")

    st.write("**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏:**")

    # –ü—Ä–µ–≤—Ä–∞—Ç–∏–º —Å–ª–æ–≤–∞—Ä—å –≤ DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    chart_data = pd.DataFrame({
        "–¢–∏–ø": [type_columns_ru[key] for key in adjusted_results.keys()],
        "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å": list(adjusted_results.values())
    })

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    st.dataframe(chart_data, use_container_width=True)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å bar chart
    st.bar_chart(chart_data.set_index("–¢–∏–ø"))

    



# st.header("Career guidance for students")
st.header("–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤")
# tabs = st.tabs(["School academic records", "Open questions"]) # , "Test ...(under development)"
tabs = st.tabs(["–®–∫–æ–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏", "–û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã", "AI –ø—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ç–æ—Ä"]) # , "–¢–µ—Å—Ç ...(–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)"

with tabs[0]:
    st.write("**–í—ã–±–µ—Ä–∏—Ç–µ —Å–≤–æ–π –º–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–∏–ø (—à–∫–æ–ª—å–Ω–∞—è –∞–Ω–∫–µ—Ç–∞ 7 –∫–ª–∞—Å—Å):**")
    # st.write("**Choose your motivation type (7th grade school questionnaire):**")
    selected_checkboxes = {}
    for col in checkbox_columns:
        selected_checkboxes[col] = st.checkbox(column_names_dict_ru[col]) # column_names_dict
    
    create_expander(7, [col for col in inp_col_names if col.endswith('_7')])
    create_expander(8, [col for col in inp_col_names if col.endswith('_8')])
    create_expander(9, [col for col in inp_col_names if col.endswith('_9')])
    create_expander(10, [col for col in inp_col_names if col.endswith('_10')])

    # if st.button("Get a result"):
    if st.button("–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"):
        df = save_to_dataframe(selected_checkboxes, input_values)
        result_df = apply_model('random_forest_model.pkl', df)
        display_results(result_df)



import requests
from dotenv import load_dotenv

load_dotenv()

my_api = st.secrets["OPENAI_API_KEY"]

def get_ai_response(answers):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {my_api}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "ft:gpt-4o-2024-08-06:personal::An4sVvnb",
        "messages": [
            {
                "role": "system",
                "content": (
                    "Assistant is an expert in career guidance. Assistant should answer in the same language "
                    "as the one in which the user writes answers (could be russian, kazakh, english). "
                    "User answers the following questions: "
                    "1. –ö–∞–∫–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç? "
                    "2. –ö–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∞–º —Ç–æ—á–Ω–æ –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã? "
                    "3. –ë–µ–∑ —É—á–µ—Ç–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –∫–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞–º –Ω—Ä–∞–≤—è—Ç—Å—è? "
                    "4. –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ —Å–≤–æ–∏ —Ö–æ–±–±–∏ –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã: "
                    "5. –ù–∞–∑–æ–≤–∏—Ç–µ —Ä–æ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏, —á–µ–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—Å –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Ç. "
                    "6. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø—Ä–∏–¥–∞—é—Ç –≤–∞–º —ç–Ω–µ—Ä–≥–∏–∏? "
                    "7. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤–∞—Å —É—Ç–æ–º–ª—è—é—Ç?"
                )
            },
            {
                "role": "user",
                "content": f"1. {answers[0]} 2. {answers[1]} 3. {answers[2]} 4. {answers[3]} 5. {answers[4]} 6. {answers[5]} 7. {answers[6]}"
            }
        ],
        "max_tokens": 500
    }

    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()  # –≤—ã–¥–∞—Å—Ç –æ—à–∏–±–∫—É –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è

    return response.json()["choices"][0]["message"]["content"]


questions_ru = [
    "**1. –ö–∞–∫–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç?**",
    "**2. –ö–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∞–º —Ç–æ—á–Ω–æ –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã?**",
    "**3. –ë–µ–∑ —É—á–µ—Ç–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –∫–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞–º –Ω—Ä–∞–≤—è—Ç—Å—è?**",
    "**4. –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ —Å–≤–æ–∏ —Ö–æ–±–±–∏ –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã:**",
    "**5. –ù–∞–∑–æ–≤–∏—Ç–µ —Ä–æ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏, —á—å–∏ –æ–±—Ä–∞–∑—ã –∂–∏–∑–Ω–∏ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—Å –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Ç.**",
    "**6. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø—Ä–∏–¥–∞—é—Ç –≤–∞–º —ç–Ω–µ—Ä–≥–∏–∏?**",
    "**7. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤–∞—Å —É—Ç–æ–º–ª—è—é—Ç?**"
]

user_answers = []

with tabs[1]:
    for i, question in enumerate(questions_ru):
        answer = st.text_input(f"{question}", key=f"answer_{i}")
        user_answers.append(answer)

    if st.button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"):
        ai_response = get_ai_response(user_answers)
        st.write("–û—Ç–≤–µ—Ç –ò–ò:")
        st.write(ai_response)


import os
import json
import streamlit as st
from huggingface_hub import InferenceClient, login
from sentence_transformers import SentenceTransformer
from annoy import AnnoyIndex


@st.cache_data(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ JSONL —Ñ–∞–π–ª–æ–≤...")
def load_jsonl_files(folder_path):
    all_records = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".jsonl"):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        all_records.append(record)
                    except json.JSONDecodeError:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {filename}")
    return all_records


rag_data = load_jsonl_files("./jsonl datafiles")


def login_hf():
    if not os.environ.get("HF_TOKEN"):
        login(token=st.secrets["HF_TOKEN"])


login_hf()


@st.cache_resource
def load_annoy_index():
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    texts = [item["text"] for item in rag_data]
    dimension = 384

    annoy_index = AnnoyIndex(dimension, 'angular')
    annoy_index.load("index.ann")

    return embedder, annoy_index, texts


embedder, annoy_index, texts = load_annoy_index()


def generate_career_advice(question: str) -> str:
    messages = [
        {"role": "system", "content":
         """–í—ã ‚Äî –∫–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¥–ª—è —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤. 
         –ï—Å–ª–∏ —É—á–µ–Ω–∏–∫ –ø—Ä–æ—Å–∏—Ç –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–∞—Ä—å–µ—Ä–Ω—ã–µ –ø—É—Ç–∏, –≤—ã–±–µ—Ä–∏—Ç–µ 3 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥ –µ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å—ã, —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è.
         –û—Ç–≤–µ—Ç –¥–µ—Ä–∂–∏—Ç–µ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 100 —Å–ª–æ–≤. –ë—É–¥—å—Ç–µ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω—ã –∏ –ø–æ –¥–µ–ª—É."""},
        {"role": "user", "content": question}
    ]

    client = InferenceClient(
        provider="auto",
        api_key=st.secrets["HF_TOKEN"]
    )

    response = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        messages=messages,
        max_tokens=350,
        temperature=0.7
    )

    return response.choices[0].message.content


def generate_rag_career_advice(question: str, embedder, annoy_index, texts: list, k: int = 5) -> str:
    query_embedding = embedder.encode([question], convert_to_numpy=True)

    indices = annoy_index.get_nns_by_vector(query_embedding[0], k, include_distances=False)
    context_docs = [texts[i] for i in indices]

    context = "\n\n".join(context_docs)

    messages = [
        {"role": "system", "content":
         f"""
–í—ã ‚Äî –∫–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¥–ª—è —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤.

–£ –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º (–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ).

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–±—Ä–∞—Ç—å 3 –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –ø—É—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∏–Ω—Ç–µ—Ä–µ—Å–∞–º, —Å–∏–ª—å–Ω—ã–º —Å—Ç–æ—Ä–æ–Ω–∞–º –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —É—á–µ–Ω–∏–∫–∞.  
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –û–ø–∏—Ä–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ —É—á–µ–Ω–∏–∫–∞, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –ª–∏—à–Ω–µ–≥–æ.  
- –î–∞–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç, –∏ –∏–∑–±–µ–≥–∞—Ç—å –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏—Ö.  
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ–±—ä—è—Å–Ω–∏—Ç—å –≤ 3‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –ø–æ—á–µ–º—É –æ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç –∏–º–µ–Ω–Ω–æ —ç—Ç–æ–º—É —É—á–µ–Ω–∏–∫—É.  
- –ù–µ –¥–∞–≤–∞—Ç—å –æ–±—â–∏—Ö —Å–æ–≤–µ—Ç–æ–≤ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ ¬´–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π¬ª.  
- –û—Ç–≤–µ—Ç –Ω–µ –±–æ–ª–µ–µ 350 —Å–ª–æ–≤.  

–ï—Å–ª–∏ —É—á–µ–Ω–∏–∫ –∑–∞–¥–∞—ë—Ç –¥—Ä—É–≥–∏–µ –≤–æ–ø—Ä–æ—Å—ã ‚Äî –æ—Ç–≤–µ—á–∞–π—Ç–µ –ø—Ä—è–º–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π—Ç–µ –∫–∞—Ä—å–µ—Ä–Ω—ã–µ –ø—É—Ç–∏.
"""},

        {"role": "user", "content": question}
    ]

    client = InferenceClient(
        provider="auto",
        api_key=st.secrets["HF_TOKEN"]
    )

    response = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        messages=messages,
        max_tokens=500,
        temperature=0.7
    )

    answer = response.choices[0].message.content

    if not answer.endswith("."):
        last_period = answer.rfind(".")
        if last_period != -1:
            answer = answer[:last_period + 1]
        else:
            answer = answer.strip()

    return answer


with tabs[2]:
    st.title("üéì AI –ö–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¥–ª—è —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤")

    student_question = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ –∫–∞—Ä—å–µ—Ä–µ:", height=100)

    use_rag = st.toggle("–í–∫–ª—é—á–∏—Ç—å RAG (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–∞–∑—ã)", value=True,
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.")

    if st.button("–ü–æ–ª—É—á–∏—Ç—å —Å–æ–≤–µ—Ç"):
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            if use_rag:
                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("üí° –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
                    base_answer = generate_career_advice(student_question)
                    st.write(base_answer)

                with col2:
                    st.subheader("üìö –ú–æ–¥–µ–ª—å —Å RAG")
                    rag_answer = generate_rag_career_advice(student_question, embedder, annoy_index, texts)
                    st.write(rag_answer)

            else:
                st.subheader("AI –°–æ–≤–µ—Ç –ø–æ –∫–∞—Ä—å–µ—Ä–µ")
                base_answer = generate_career_advice(student_question)
                st.write(base_answer)