import streamlit as st
import pandas as pd
import numpy as np
import joblib
import requests
import os
import json
import logging
from dotenv import load_dotenv
from huggingface_hub import InferenceClient, login
from sentence_transformers import SentenceTransformer
from annoy import AnnoyIndex
from streamlit_option_menu import option_menu


# ==========================
#  CONFIG
# ==========================
st.set_page_config(page_title="–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤")
logging.basicConfig(level=logging.DEBUG)
load_dotenv()


# ==========================
#  GLOBAL DATA
# ==========================
out_col_names = ['signsystem', 'technology', 'nature', 'artistic', 'human', 'business']
inp_col_names = [
    'kaz_lang_7', 'liter_7', 'rus_lang_7', 'eng_lang_7', 'math_7', 'comps_7',
    'kaz_hist_7', 'art_7', 'pe_7', 'geography_7', 'biology_7', 'chemistry_7',
    'physics_7', 'world_hist_7', 'Activist', 'Career', 'Tester', 'Creator',
    'Designer', 'Researcher', 'kaz_lang_8', 'liter_8', 'rus_lang_8', 'eng_lang_8',
    'math_8', 'comps_8', 'kaz_hist_8', 'art_8', 'pe_8', 'geography_8', 'biology_8',
    'chemistry_8', 'physics_8', 'world_hist_8', 'kaz_lang_9', 'liter_9', 'rus_lang_9',
    'eng_lang_9', 'math_9', 'comps_9', 'kaz_hist_9', 'art_9', 'pe_9', 'geography_9',
    'biology_9', 'chemistry_9', 'physics_9', 'world_hist_9', 'rights_9', 'kaz_lang_10',
    'liter_10', 'rus_lang_10', 'eng_lang_10', 'math_10', 'comps_10', 'kaz_hist_10',
    'art_10', 'pe_10', 'geography_10', 'biology_10', 'chemistry_10', 'physics_10',
    'world_hist_10'
]

# (—Å–æ–∫—Ä–∞—â–µ–Ω–æ: dict-—ã column_names_dict –∏ column_names_dict_ru, checkbox_columns, type_columns, thresholds –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
column_names_dict = { 'kaz_lang_7': 'Kazakh Language', 'liter_7': 'Literature', 'rus_lang_7': 'Russian Language', 'eng_lang_7': 'English Language', 'math_7': 'Mathematics', 'comps_7': 'Informatics', 'kaz_hist_7': 'History of Kazakhstan', 'art_7': 'Art', 'pe_7': 'Physical Education', 'geography_7': 'Geography', 'biology_7': 'Biology', 'chemistry_7': 'Chemistry', 'physics_7': 'Physics', 'world_hist_7': 'World History', 'Activist': 'Activist', 'Career': 'Careerist', 'Tester': 'Tester', 'Creator': 'Creator', 'Designer': 'Designer', 'Researcher': 'Researcher', 'kaz_lang_8': 'Kazakh Language', 'liter_8': 'Literature', 'rus_lang_8': 'Russian Language', 'eng_lang_8': 'English Language', 'math_8': 'Mathematics', 'comps_8': 'Informatics', 'kaz_hist_8': 'History of Kazakhstan', 'art_8': 'Art', 'pe_8': 'Physical Education', 'geography_8': 'Geography', 'biology_8': 'Biology', 'chemistry_8': 'Chemistry', 'physics_8': 'Physics', 'world_hist_8': 'World History', 'kaz_lang_9': 'Kazakh Language', 'liter_9': 'Literature', 'rus_lang_9': 'Russian Language', 'eng_lang_9': 'English Language', 'math_9': 'Mathematics', 'comps_9': 'Informatics', 'kaz_hist_9': 'History of Kazakhstan', 'art_9': 'Art', 'pe_9': 'Physical Education', 'geography_9': 'Geography', 'biology_9': 'Biology', 'chemistry_9': 'Chemistry', 'physics_9': 'Physics', 'world_hist_9': 'World History', 'rights_9': 'Law Fundamentals', 'kaz_lang_10': 'Kazakh Language', 'liter_10': 'Literature', 'rus_lang_10': 'Russian Language', 'eng_lang_10': 'English Language', 'math_10': 'Mathematics', 'comps_10': 'Informatics', 'kaz_hist_10': 'History of Kazakhstan', 'art_10': 'Art', 'pe_10': 'Physical Education', 'geography_10': 'Geography', 'biology_10': 'Biology', 'chemistry_10': 'Chemistry', 'physics_10': 'Physics', 'world_hist_10': 'World History' } 
column_names_dict_ru = { 'kaz_lang_7': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫', 'liter_7': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'rus_lang_7': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫', 'eng_lang_7': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫', 'math_7': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞', 'comps_7': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 'kaz_hist_7': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞', 'art_7': '–ò—Å–∫—É—Å—Å—Ç–≤–æ', 'pe_7': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞', 'geography_7': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è', 'biology_7': '–ë–∏–æ–ª–æ–≥–∏—è', 'chemistry_7': '–•–∏–º–∏—è', 'physics_7': '–§–∏–∑–∏–∫–∞', 'world_hist_7': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è', 'Activist': '–ê–∫—Ç–∏–≤–∏—Å—Ç', 'Career': '–ö–∞—Ä—å–µ—Ä–∏—Å—Ç', 'Tester': '–ò—Å–ø—ã—Ç–∞—Ç–µ–ª—å', 'Creator': '–¢–≤–æ—Ä–µ—Ü', 'Designer': '–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫', 'Researcher': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å', 'kaz_lang_8': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫', 'liter_8': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'rus_lang_8': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫', 'eng_lang_8': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫', 'math_8': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞', 'comps_8': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 'kaz_hist_8': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞', 'art_8': '–ò—Å–∫—É—Å—Å—Ç–≤–æ', 'pe_8': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞', 'geography_8': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è', 'biology_8': '–ë–∏–æ–ª–æ–≥–∏—è', 'chemistry_8': '–•–∏–º–∏—è', 'physics_8': '–§–∏–∑–∏–∫–∞', 'world_hist_8': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è', 'kaz_lang_9': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫', 'liter_9': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'rus_lang_9': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫', 'eng_lang_9': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫', 'math_9': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞', 'comps_9': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 'kaz_hist_9': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞', 'art_9': '–ò—Å–∫—É—Å—Å—Ç–≤–æ', 'pe_9': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞', 'geography_9': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è', 'biology_9': '–ë–∏–æ–ª–æ–≥–∏—è', 'chemistry_9': '–•–∏–º–∏—è', 'physics_9': '–§–∏–∑–∏–∫–∞', 'world_hist_9': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è', 'rights_9': '–û—Å–Ω–æ–≤—ã –ø—Ä–∞–≤–∞', 'kaz_lang_10': '–ö–∞–∑–∞—Ö—Å–∫–∏–π —è–∑—ã–∫', 'liter_10': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 'rus_lang_10': '–†—É—Å—Å–∫–∏–π —è–∑—ã–∫', 'eng_lang_10': '–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫', 'math_10': '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞', 'comps_10': '–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 'kaz_hist_10': '–ò—Å—Ç–æ—Ä–∏—è –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞', 'art_10': '–ò—Å–∫—É—Å—Å—Ç–≤–æ', 'pe_10': '–§–∏–∑–∫—É–ª—å—Ç—É—Ä–∞', 'geography_10': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—è', 'biology_10': '–ë–∏–æ–ª–æ–≥–∏—è', 'chemistry_10': '–•–∏–º–∏—è', 'physics_10': '–§–∏–∑–∏–∫–∞', 'world_hist_10': '–í—Å–µ–º–∏—Ä–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è' } 
checkbox_columns = [ 'Activist', 'Career', 'Tester', 'Creator', 'Designer', 'Researcher' ]
type_columns = { 'class_0': 'Person-Sign System', 'class_1': 'Person-Technology', 'class_2': 'Person-Nature', 'class_3': 'Person-Artistic Image', 'class_4': 'Person-Person', 'class_5': 'Person-Business' } 
type_columns_ru = { 'class_0': '–ß–µ–ª–æ–≤–µ–∫-–ó–Ω–∞–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞', 'class_1': '–ß–µ–ª–æ–≤–µ–∫-–¢–µ—Ö–Ω–∏–∫–∞', 'class_2': '–ß–µ–ª–æ–≤–µ–∫-–ü—Ä–∏—Ä–æ–¥–∞', 'class_3': '–ß–µ–ª–æ–≤–µ–∫-–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑', 'class_4': '–ß–µ–ª–æ–≤–µ–∫-–ß–µ–ª–æ–≤–µ–∫', 'class_5': '–ß–µ–ª–æ–≤–µ–∫-–ë–∏–∑–Ω–µ—Å' }
thresholds = { 'class_0': 0.39, 'class_1': 0.30903005409623036, 'class_2': 0.23611111111111113, 'class_3': 0.44833333333333336, 'class_4': 0.13, 'class_5': 0.17 }

questions_ru = [ "**1. –ö–∞–∫–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç?**", "**2. –ö–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∞–º —Ç–æ—á–Ω–æ –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã?**", "**3. –ë–µ–∑ —É—á–µ—Ç–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤, –∫–∞–∫–∏–µ –≤–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤–∞–º –Ω—Ä–∞–≤—è—Ç—Å—è?**", "**4. –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ —Å–≤–æ–∏ —Ö–æ–±–±–∏ –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã:**", "**5. –ù–∞–∑–æ–≤–∏—Ç–µ —Ä–æ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏, —á—å–∏ –æ–±—Ä–∞–∑—ã –∂–∏–∑–Ω–∏ –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤–∞—Å –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Ç.**", "**6. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø—Ä–∏–¥–∞—é—Ç –≤–∞–º —ç–Ω–µ—Ä–≥–∏–∏?**", "**7. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤–∞—Å —É—Ç–æ–º–ª—è—é—Ç?**" ]

# ==========================
#  HELPER FUNCTIONS
# ==========================
def create_expander(class_label, cols):
    with st.expander(f"–û—Ü–µ–Ω–∫–∏ –∑–∞ {class_label} –∫–ª–∞—Å—Å"):
        for col in cols:
            input_values[col] = st.number_input(
                column_names_dict_ru[col], min_value=2, max_value=5, step=1, value=5, key=col
            )

def save_to_dataframe(selected_checkboxes, input_values):
    data = {**selected_checkboxes, **input_values}
    for key in checkbox_columns:
        data[key] = int(data.get(key, False))
    df = pd.DataFrame([data], columns=inp_col_names)
    return df

def apply_model(model_path, input_df):
    model = joblib.load(model_path)
    probabilities = model.predict_proba(input_df)
    if isinstance(probabilities, list):
        probabilities = np.array(probabilities)
    probability_dict = {f'class_{i}': probabilities[i][:, 1] for i in range(len(probabilities))}
    return pd.DataFrame(probability_dict)

def adjust_probabilities(probabilities, thresholds):
    return {key: min(100, (val / thresholds[key]) * 100) for key, val in probabilities.items()}

def display_results(df):
    results = {key: df[key].values[0] for key in df.columns}
    adjusted = adjust_probabilities(results, thresholds)
    selected_types = [type_columns_ru[k] for k, v in adjusted.items() if v >= 100]

    st.write("**–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–∏–ø—ã:**")
    for t in selected_types:
        st.write(f"- {t}")

    chart_data = pd.DataFrame({
        "–¢–∏–ø": [type_columns_ru[k] for k in adjusted.keys()],
        "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å": list(adjusted.values())
    })
    st.dataframe(chart_data, use_container_width=True)
    st.bar_chart(chart_data.set_index("–¢–∏–ø"))


def get_ai_response(answers):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {st.secrets['OPENAI_API_KEY']}", "Content-Type": "application/json"}
    data = {
        "model": "ft:gpt-4o-2024-08-06:personal::An4sVvnb",
        "messages": [
            {"role": "system", "content": "Assistant is an expert in career guidance..."},
            {"role": "user", "content": " ".join([f"{i+1}. {a}" for i, a in enumerate(answers)])}
        ],
        "max_tokens": 500
    }
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

@st.cache_data(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ JSONL —Ñ–∞–π–ª–æ–≤...")
def load_jsonl_files(folder_path):
    records = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".jsonl"):
            with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        records.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
    return records

def login_hf():
    if not os.environ.get("HF_TOKEN"):
        login(token=st.secrets["HF_TOKEN"])

@st.cache_resource
def load_annoy_index(rag_data):
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    texts = [item["text"] for item in rag_data]
    index = AnnoyIndex(384, 'angular')
    index.load("index.ann")
    return embedder, index, texts

def generate_career_advice(question: str):
    client = InferenceClient(provider="auto", api_key=st.secrets["HF_TOKEN"])
    response = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        messages=[{"role": "system", "content": "–í—ã ‚Äî –∫–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç"}, {"role": "user", "content": question}],
        max_tokens=350, temperature=0.7
    )
    return response.choices[0].message.content

def generate_rag_career_advice(question: str, embedder, index, texts, k=5):
    q_emb = embedder.encode([question], convert_to_numpy=True)[0]
    ctx = "\n\n".join([texts[i] for i in index.get_nns_by_vector(q_emb, k)])
    client = InferenceClient(provider="auto", api_key=st.secrets["HF_TOKEN"])
    response = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        messages=[
            {"role": "system", "content": f"–í—ã –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{ctx}"},
            {"role": "user", "content": question}
        ],
        max_tokens=500, temperature=0.7
    )
    return response.choices[0].message.content


# ==========================
#  INTERFACE
# ==========================
input_values = {}
rag_data = load_jsonl_files("./jsonl datafiles")
login_hf()
embedder, annoy_index, texts = load_annoy_index(rag_data)

st.header("–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤")
tabs = st.tabs(["–®–∫–æ–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏", "–û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã", "AI –ø—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ç–æ—Ä"])

# TAB 1
with tabs[0]:
    st.write("**–í—ã–±–µ—Ä–∏—Ç–µ —Å–≤–æ–π –º–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–∏–ø:**")
    selected_checkboxes = {col: st.checkbox(column_names_dict_ru[col]) for col in checkbox_columns}
    for grade in [7, 8, 9, 10]:
        create_expander(grade, [c for c in inp_col_names if c.endswith(f"_{grade}")])

    if st.button("–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç"):
        df = save_to_dataframe(selected_checkboxes, input_values)
        result_df = apply_model("random_forest_model.pkl", df)
        st.session_state["tab1_results"] = result_df

    if "tab1_results" in st.session_state:
        display_results(st.session_state["tab1_results"])


# TAB 2
with tabs[1]:
    user_answers = [st.text_input(q, key=f"answer_{i}") for i, q in enumerate(questions_ru)]
    if st.button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"):
        ai_response = get_ai_response(user_answers)
        st.session_state["tab2_ai_response"] = ai_response

    if "tab2_ai_response" in st.session_state:
        st.write("–û—Ç–≤–µ—Ç –ò–ò:")
        st.write(st.session_state["tab2_ai_response"])


# TAB 3
with tabs[2]:
    st.title("üéì AI –ö–∞—Ä—å–µ—Ä–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç")
    student_question = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:", height=100, key="student_q")
    use_rag = st.toggle("–í–∫–ª—é—á–∏—Ç—å RAG", value=True)

    if st.button("–ü–æ–ª—É—á–∏—Ç—å —Å–æ–≤–µ—Ç"):
        if use_rag:
            st.session_state["tab3_base"] = generate_career_advice(student_question)
            st.session_state["tab3_rag"] = generate_rag_career_advice(student_question, embedder, annoy_index, texts)
        else:
            st.session_state["tab3_base"] = generate_career_advice(student_question)

    if "tab3_base" in st.session_state:
        st.subheader("üí° –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        st.write(st.session_state["tab3_base"])
    if use_rag and "tab3_rag" in st.session_state:
        st.subheader("üìö –ú–æ–¥–µ–ª—å —Å RAG")
        st.write(st.session_state["tab3_rag"])
